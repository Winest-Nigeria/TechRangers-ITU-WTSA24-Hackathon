{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "<img src=\"https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F769452%2Fb18d0513200d426e556b2b7b7c825981%2FRAG.png?generation=1695504022336680&alt=media\"></img>\n",
    "\n",
    "## Objective\n",
    "\n",
    "Use Llama 2.0, Langchain and ChromaDB to create a Retrieval Augmented Generation (RAG) system. This will allow us to ask questions about our documents (that were not included in the training data), without fine-tunning the Large Language Model (LLM).\n",
    "When using RAG, if you are given a question, you first do a retrieval step to fetch any relevant documents from a special database, a vector database where these documents were indexed. \n",
    "\n",
    "## Definitions\n",
    "\n",
    "* LLM - Large Language Model  \n",
    "* Llama 2.0 - LLM from Meta \n",
    "* Langchain - a framework designed to simplify the creation of applications using LLMs\n",
    "* Vector database - a database that organizes data through high-dimmensional vectors  \n",
    "* ChromaDB - vector database  \n",
    "* RAG - Retrieval Augmented Generation (see below more details about RAGs)\n",
    "\n",
    "## Model details\n",
    "\n",
    "* **Model**: Llama 2  \n",
    "* **Variation**: 7b-chat-hf  (7b: 7B dimm. hf: HuggingFace build)\n",
    "* **Version**: V1  \n",
    "* **Framework**: PyTorch  \n",
    "\n",
    "LlaMA 2 model is pretrained and fine-tuned with 2 Trillion tokens and 7 to 70 Billion parameters which makes it one of the powerful open source models. It is a highly improvement over LlaMA 1 model.\n",
    "\n",
    "\n",
    "## What is a Retrieval Augmented Generation (RAG) system?\n",
    "\n",
    "Large Language Models (LLMs) has proven their ability to understand context and provide accurate answers to various NLP tasks, including summarization, Q&A, when prompted. While being able to provide very good answers to questions about information that they were trained with, they tend to hallucinate when the topic is about information that they do \"not know\", i.e. was not included in their training data. Retrieval Augmented Generation combines external resources with LLMs. The main two components of a RAG are therefore a retriever and a generator.  \n",
    " \n",
    "The retriever part can be described as a system that is able to encode our data so that can be easily retrieved the relevant parts of it upon queriying it. The encoding is done using text embeddings, i.e. a model trained to create a vector representation of the information. The best option for implementing a retriever is a vector database. As vector database, there are multiple options, both open source or commercial products. Few examples are ChromaDB, Mevius, FAISS, Pinecone, Weaviate. Our option in this Notebook will be a local instance of ChromaDB (persistent).\n",
    "\n",
    "For the generator part, the obvious option is a LLM. In this Notebook we will use a quantized LLaMA v2 model, from the Kaggle Models collection.  \n",
    "\n",
    "The orchestration of the retriever and generator will be done using Langchain. A specialized function from Langchain allows us to create the receiver-generator in one line of code.\n",
    "\n",
    "## More about this  \n",
    "\n",
    "Do you want to learn more? Look into the `References` section for blog posts and in `More work on the same topic` for Notebooks about the technologies used here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installations, imports, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.33.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (4.33.0)\n",
      "Requirement already satisfied: accelerate==0.22.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (0.22.0)\n",
      "Requirement already satisfied: einops==0.6.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (0.6.1)\n",
      "Requirement already satisfied: langchain==0.0.300 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (0.0.300)\n",
      "Requirement already satisfied: xformers==0.0.21 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (0.0.21)\n",
      "Requirement already satisfied: bitsandbytes==0.41.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (0.41.1)\n",
      "Requirement already satisfied: sentence_transformers==2.2.2 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: chromadb==0.4.12 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (0.4.12)\n",
      "Requirement already satisfied: filelock in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from transformers==4.33.0) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from transformers==4.33.0) (0.24.5)\n",
      "Requirement already satisfied: numpy>=1.17 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from transformers==4.33.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from transformers==4.33.0) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from transformers==4.33.0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from transformers==4.33.0) (2024.7.24)\n",
      "Requirement already satisfied: requests in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from transformers==4.33.0) (2.32.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from transformers==4.33.0) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from transformers==4.33.0) (0.4.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from transformers==4.33.0) (4.66.5)\n",
      "Requirement already satisfied: psutil in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from accelerate==0.22.0) (6.0.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from accelerate==0.22.0) (2.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from langchain==0.0.300) (2.0.35)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from langchain==0.0.300) (3.10.9)\n",
      "Requirement already satisfied: anyio<4.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from langchain==0.0.300) (3.7.1)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from langchain==0.0.300) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from langchain==0.0.300) (0.6.7)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from langchain==0.0.300) (1.33)\n",
      "Requirement already satisfied: langsmith<0.1.0,>=0.0.38 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from langchain==0.0.300) (0.0.92)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from langchain==0.0.300) (2.10.1)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from langchain==0.0.300) (1.10.18)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from langchain==0.0.300) (8.5.0)\n",
      "Requirement already satisfied: torchvision in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from sentence_transformers==2.2.2) (0.15.2)\n",
      "Requirement already satisfied: scikit-learn in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from sentence_transformers==2.2.2) (1.5.2)\n",
      "Requirement already satisfied: scipy in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from sentence_transformers==2.2.2) (1.14.1)\n",
      "Requirement already satisfied: nltk in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from sentence_transformers==2.2.2) (3.9.1)\n",
      "Requirement already satisfied: sentencepiece in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from sentence_transformers==2.2.2) (0.2.0)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.3 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from chromadb==0.4.12) (0.7.3)\n",
      "Requirement already satisfied: fastapi<0.100.0,>=0.95.2 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from chromadb==0.4.12) (0.99.1)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (0.31.0)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from chromadb==0.4.12) (3.7.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from chromadb==0.4.12) (4.12.2)\n",
      "Requirement already satisfied: pulsar-client>=3.1.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from chromadb==0.4.12) (3.5.0)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from chromadb==0.4.12) (1.19.2)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from chromadb==0.4.12) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from chromadb==0.4.12) (7.7.0)\n",
      "Requirement already satisfied: importlib-resources in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from chromadb==0.4.12) (6.4.5)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from chromadb==0.4.12) (4.2.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from chromadb==0.4.12) (0.12.3)\n",
      "Requirement already satisfied: sympy in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (1.13.2)\n",
      "Requirement already satisfied: networkx in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (11.7.101)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (10.9.0.58)\n",
      "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (10.2.10.91)\n",
      "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (11.4.0.1)\n",
      "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (11.7.4.91)\n",
      "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (2.14.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (11.7.91)\n",
      "Requirement already satisfied: triton==2.0.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (2.0.0)\n",
      "Requirement already satisfied: setuptools in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate==0.22.0) (72.1.0)\n",
      "Requirement already satisfied: wheel in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.10.0->accelerate==0.22.0) (0.43.0)\n",
      "Requirement already satisfied: cmake in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.10.0->accelerate==0.22.0) (3.30.4)\n",
      "Requirement already satisfied: lit in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from triton==2.0.0->torch>=1.10.0->accelerate==0.22.0) (18.1.8)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (1.14.0)\n",
      "Requirement already satisfied: idna>=2.8 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from anyio<4.0->langchain==0.0.300) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from anyio<4.0->langchain==0.0.300) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from anyio<4.0->langchain==0.0.300) (1.2.2)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.300) (3.22.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.300) (0.9.0)\n",
      "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from fastapi<0.100.0,>=0.95.2->chromadb==0.4.12) (0.27.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.33.0) (2024.6.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain==0.0.300) (3.0.0)\n",
      "Requirement already satisfied: coloredlogs in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.12) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.12) (24.3.25)\n",
      "Requirement already satisfied: protobuf in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.12) (5.28.2)\n",
      "Requirement already satisfied: six>=1.5 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb==0.4.12) (1.16.0)\n",
      "Requirement already satisfied: monotonic>=1.5 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb==0.4.12) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb==0.4.12) (2.2.1)\n",
      "Requirement already satisfied: python-dateutil>2.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb==0.4.12) (2.9.0.post0)\n",
      "Requirement already satisfied: certifi in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from pulsar-client>=3.1.0->chromadb==0.4.12) (2024.7.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from requests->transformers==4.33.0) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from requests->transformers==4.33.0) (2.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.300) (3.1.1)\n",
      "Requirement already satisfied: click>=8.0.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from typer>=0.9.0->chromadb==0.4.12) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from typer>=0.9.0->chromadb==0.4.12) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from typer>=0.9.0->chromadb==0.4.12) (13.7.1)\n",
      "Requirement already satisfied: h11>=0.8 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb==0.4.12) (0.14.0)\n",
      "Requirement already satisfied: httptools>=0.5.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (0.6.1)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (1.0.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (0.20.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (0.24.0)\n",
      "Requirement already satisfied: websockets>=10.4 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (13.1)\n",
      "Requirement already satisfied: joblib in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from nltk->sentence_transformers==2.2.2) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from scikit-learn->sentence_transformers==2.2.2) (3.5.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from torchvision->sentence_transformers==2.2.2) (10.4.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb==0.4.12) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb==0.4.12) (2.18.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.300) (1.0.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (0.2.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb==0.4.12) (10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.22.0) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate==0.22.0) (1.3.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb==0.4.12) (0.1.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install transformers==4.33.0 accelerate==0.22.0 einops==0.6.1 langchain==0.0.300 xformers==0.0.21 \\\n",
    "bitsandbytes==0.41.1 sentence_transformers==2.2.2 chromadb==0.4.12\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "from torch import cuda, bfloat16\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from time import time\n",
    "#import chromadb\n",
    "#from chromadb.config import Settings\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Chroma\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize model, tokenizer, query pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model, the device, and the `bitsandbytes` configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "model_id = 'meta-llama/Llama-2-7b-chat-hf'\n",
    "\n",
    "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
    "\n",
    "# set quantization configuration to load large model with less GPU memory\n",
    "# this requires the `bitsandbytes` library\n",
    "bnb_config = transformers.BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type='nf4',\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved to /home/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Replace 'your_hugging_face_token' with your actual Hugging Face token\n",
    "token = \"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\"\n",
    "\n",
    "# Log in to Hugging Face\n",
    "login(token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the model and the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adfea1b2fd1a44d2a94d51a75e254e4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/py3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare model, tokenizer: 68.693 sec.\n"
     ]
    }
   ],
   "source": [
    "time_1 = time()\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "time_2 = time()\n",
    "print(f\"Prepare model, tokenizer: {round(time_2-time_1, 3)} sec.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the query pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare pipeline: 0.425 sec.\n"
     ]
    }
   ],
   "source": [
    "time_1 = time()\n",
    "query_pipeline = transformers.pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\",)\n",
    "time_2 = time()\n",
    "print(f\"Prepare pipeline: {round(time_2-time_1, 3)} sec.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_code_prompt(user_input):\n",
    "    # You can expand this list with more code-related keywords or programming languages\n",
    "    code_keywords = [\"code\", \"script\", \"function\", \"class\", \"method\", \"algorithm\", \"program\", \"loop\", \"Python\", \"JavaScript\", \"C++\", \"Java\"]\n",
    "    \n",
    "    return any(keyword.lower() in user_input.lower() for keyword in code_keywords)\n",
    "\n",
    "# Define a function to handle code generation prompts\n",
    "def handle_code_prompt(pipeline, tokenizer, prompt_to_test):\n",
    "    print(f\"Generating code for the prompt: {prompt_to_test}\")\n",
    "    \n",
    "    # Generate code using the pipeline\n",
    "    time_1 = time()\n",
    "    sequences = pipeline(\n",
    "        prompt_to_test,\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_length=200\n",
    "    )\n",
    "    time_2 = time()\n",
    "    \n",
    "    print(f\"Code generation time: {round(time_2-time_1, 3)} sec.\")\n",
    "    for seq in sequences:\n",
    "        print(f\"Generated Code: {seq['generated_text']}\")\n",
    "    \n",
    "    # Optionally allow editing or further interaction with the code\n",
    "    edit_code = input(\"\\nDo you want to edit the code? (yes/no): \")\n",
    "    if edit_code.lower() == 'yes':\n",
    "        print(\"\\nYou can now edit the code manually.\")\n",
    "\n",
    "# Function to test the model with a generic prompt\n",
    "def test_model(tokenizer, pipeline, prompt_to_test):\n",
    "    print(f\"Processing query: {prompt_to_test}\")\n",
    "    \n",
    "    if is_code_prompt(prompt_to_test):\n",
    "        # Handle code-specific prompt\n",
    "        handle_code_prompt(pipeline, tokenizer, prompt_to_test)\n",
    "    else:\n",
    "        # Handle non-code queries\n",
    "        time_1 = time()\n",
    "        sequences = pipeline(\n",
    "            prompt_to_test,\n",
    "            do_sample=True,\n",
    "            top_k=10,\n",
    "            num_return_sequences=1,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            max_length=200,\n",
    "        )\n",
    "        time_2 = time()\n",
    "        \n",
    "        print(f\"Test inference: {round(time_2-time_1, 3)} sec.\")\n",
    "        for seq in sequences:\n",
    "            print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function for testing the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the query pipeline\n",
    "\n",
    "We test the pipeline with a query about the meaning of State of the Union (SOTU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing query: Please explain what is Adaptation of communication system based on changing user accessibility needs. Give just a definition. Keep it in 100 words.\n",
      "Test inference: 50.094 sec.\n",
      "Result: Please explain what is Adaptation of communication system based on changing user accessibility needs. Give just a definition. Keep it in 100 words.\n",
      "Adaptation of communication systems based on changing user accessibility needs involves modifying or customizing the system to accommodate individuals with varying levels of accessibility requirements, such as visual, auditory, motor, or cognitive impairments. This may involve using assistive technologies, such as screen readers, speech-to-text software, or Braille displays, to enable individuals with disabilities to access and use the system effectively.\n"
     ]
    }
   ],
   "source": [
    "test_model(tokenizer,\n",
    "           query_pipeline,\n",
    "           \"Please explain what is Adaptation of communication system based on changing user accessibility needs. Give just a definition. Keep it in 100 words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T19:22:16.434937Z",
     "iopub.status.busy": "2023-09-23T19:22:16.433666Z",
     "iopub.status.idle": "2023-09-23T19:22:16.440864Z",
     "shell.execute_reply": "2023-09-23T19:22:16.439217Z",
     "shell.execute_reply.started": "2023-09-23T19:22:16.434891Z"
    }
   },
   "source": [
    "## Check the model with a HuggingFace pipeline\n",
    "\n",
    "\n",
    "We check the model with a HF pipeline, using a query about the meaning of State of the Union (SOTU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMachine Learning for Network Automation involves using algorithms and statistical models to analyze network data and automatically learn patterns, behaviors, and anomalies.ibility to predict network behavior, optimize network performance, and detect and respond to security threats.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm = HuggingFacePipeline(pipeline=query_pipeline)\n",
    "# checking again that everything is working fine\n",
    "llm(prompt=\"Please explain Machine Learning for Network Automation. Give just a definition. Keep it in 100 words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingestion of data using Text loder\n",
    "\n",
    "We will ingest the newest presidential address, from Jan 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "loader = TextLoader(\"train data.txt\",\n",
    "                    encoding=\"ISO-8859-1\")\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data in chunks\n",
    "\n",
    "We split data in chunks using a recursive character text splitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "all_splits = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Embeddings and Storing in Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the embeddings using Sentence Transformer and HuggingFace embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {\"device\": \"cuda\"}\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize ChromaDB with the document splits, the embeddings defined previously and with the option to persist it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(documents=all_splits, embedding=embeddings, persist_directory=\"chroma_db\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "retriever = vectordb.as_retriever()\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    chain_type=\"stuff\", \n",
    "    retriever=retriever, \n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Retrieval-Augmented Generation \n",
    "\n",
    "\n",
    "We define a test function, that will run the query and time it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [],
   "source": [
    "def test_rag(qa, query):\n",
    "    print(f\"Query: {query}\\n\")\n",
    "    time_1 = time()\n",
    "    result = qa.run(query)\n",
    "    time_2 = time()\n",
    "    print(f\"Inference time: {round(time_2-time_1, 3)} sec.\")\n",
    "    print(\"\\nResult: \", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check few queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Given the frequent call drops experienced at my university, which utilizes an XSeries server and a star topology network, what specific recommendations would you propose to improve call quality?\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Inference time: 243.128 sec.\n",
      "\n",
      "Result:   Thank you for reaching out! To improve call quality in your university's network, I would recommend a few specific actions based on the information provided:\n",
      "\n",
      "1. Conduct Network Troubleshooting: Before making any recommendations, it's essential to identify the root cause of the call drops. I would suggest conducting a thorough network troubleshooting exercise to determine if there are any issues with the network infrastructure, such as poor signal strength, interference, or congestion.\n",
      "2. Optimize Network Configuration: Once the network troubleshooting exercise is complete, you can optimize the network configuration to improve call quality. This may involve adjusting the network settings, such as the transmit power, antenna orientation, and signal-to-noise ratio (SNR) settings.\n",
      "3. Implement Quality of Service (QoS) Policies: QoS policies can help prioritize traffic and ensure that critical applications, such as voice and video, receive adequate bandwidth and low latency. By implementing QoS policies, you can ensure that call quality is maintained, even during times of high network traffic.\n",
      "4. Enhance Network Security: Ensuring the security of your network is crucial for maintaining call quality. I would recommend implementing robust security measures, such as firewalls, intrusion detection systems, and encryption, to protect the network from unauthorized access and malicious attacks.\n",
      "5. Monitor Network Performance: Regular monitoring of network performance can help identify issues before they impact call quality. I would suggest implementing network monitoring tools to track key performance indicators (KPIs) such as latency, packet loss, and jitter, and to identify areas for improvement.\n",
      "\n",
      "By implementing these recommendations, you can significantly improve call quality in your university's network. If you have any further questions or concerns, please feel free to ask!\n"
     ]
    }
   ],
   "source": [
    "query = \"Given the frequent call drops experienced at my university, which utilizes an XSeries server and a star topology network, what specific recommendations would you propose to improve call quality?\"\n",
    "test_rag(qa, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Develop an algorithm to implemnt the recommendations you have provided\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Inference time: 160.244 sec.\n",
      "\n",
      "Result:  \n",
      "To implement the recommendations provided, we can develop an algorithm that leverages the knowledge graph and intelligent recommendation algorithms to identify the root cause of network abnormalities and provide recommended solutions to first-line experts. Here's a high-level outline of the algorithm:\n",
      "\n",
      "1. Data Collection: Collect tens of thousands of expert experiences, radio network knowledge bases, and abnormal condition data from the network.\n",
      "2. Knowledge Graph Construction: Build a knowledge graph using the collected data to establish relationships between network elements, abnormal conditions, and recommended solutions.\n",
      "3. Intelligent Recommendation Algorithm Development: Develop an intelligent recommendation algorithm that can analyze the knowledge graph and provide reasons and recommended solutions for each abnormal condition.\n",
      "4. Search Engine Development: Develop a search engine that can quickly search the knowledge graph for relevant information and provide the recommended solutions to first-line experts.\n",
      "5. Integration with First-Line Experts: Integrate the recommendation algorithm and search engine with the first-line experts' workflow to provide real-time recommendations and improve troubleshooting efficiency.\n",
      "6. Continuous Improvement: Continuously evaluate the effectiveness of the recommendation algorithm and knowledge graph, and optimize them based on the feedback from first-line experts.\n",
      "\n",
      "This algorithm can help reduce troubleshooting time and error rate by providing accurate and relevant recommended solutions to first-line experts, and improve the overall efficiency of the network maintenance process.\n"
     ]
    }
   ],
   "source": [
    "query = \"Develop an algorithm to implemnt the recommendations you have provided\"\n",
    "test_rag(qa, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: generate a python code any existing an existing network\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Inference time: 324.314 sec.\n",
      "\n",
      "Result:   I can provide you with some guidance on how to generate a Python code for creating a digital twin of an existing network. However, I must inform you that creating a digital twin of a complex network like a telecommunications network is a challenging task that requires significant expertise in network engineering, software development, and data analytics.\n",
      "\n",
      "To start, you will need to gather information about the existing network, including its architecture, topology, and performance metrics. This information can be obtained through various means, such as network monitoring tools, network management systems, and performance monitoring systems.\n",
      "\n",
      "Once you have gathered the necessary information, you can use Python programming language to create a digital twin of the network. There are several libraries and frameworks available in Python that can help you with this task, including:\n",
      "\n",
      "1. NetworkX: A Python library for creating and manipulating complex networks.\n",
      "2. PySAL: A Python library for analyzing and manipulating large-scale network data.\n",
      "3. NetworkFlow: A Python library for modeling and analyzing network traffic.\n",
      "4. PyTorch: A Python library for machine learning and artificial intelligence.\n",
      "\n",
      "You can use these libraries to create a digital twin of the existing network by defining the network architecture, adding nodes and edges to represent the network components and their connections, and defining the network's behavior and performance metrics.\n",
      "\n",
      "Here is an example of how you could use NetworkX library to create a digital twin of a simple network:\n",
      "```\n",
      "import networkx as nx\n",
      "\n",
      "# Create a directed graph representing the network\n",
      "G = nx.DiGraph()\n",
      "\n",
      "# Add nodes and edges to represent the network components and their connections\n",
      "G.add_nodes_from(['Node1', 'Node2', 'Node3'])\n",
      "G.add_edges_from([('Node1', 'Node2'), ('Node2', 'Node3'), ('Node3', 'Node1')])\n",
      "\n",
      "# Define the network's behavior and performance metrics\n",
      "G.set_edge_attributes('weight', {'from': 1, 'to': 2})\n",
      "G.set_node_attributes('label', {'Node1': 'Server', 'Node2': 'Client', 'Node3': 'Firewall'})\n",
      "\n",
      "# Print the digital twin of the network\n",
      "print(nx.draw_networkx(G, with_labels=True))\n",
      "```\n",
      "This code creates a simple network with three nodes and three edges, and defines the network's behavior and performance metrics. You can modify this code to create a digital twin of a more complex network, such as a telecommunications network, by adding more nodes and edges and defining more detailed network behavior and performance metrics.\n",
      "\n",
      "I hope this helps! Let me know if you have any further questions.\n"
     ]
    }
   ],
   "source": [
    "query = \"generate a python code any existing an existing network\"\n",
    "test_rag(qa, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document sources\n",
    "\n",
    "Let's check the documents sources, for the last query run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_kg_hide-input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: generate a python code any existing an existing network\n",
      "Retrieved documents: 4\n",
      "Source:  train data.txt\n",
      "Text:  OR\n",
      "Top down bootstrapping of apps, services, NaaS, infrastructure, based on these\n",
      "new SRCs.\n",
      "Open issues E2E automation frameworks for composition of infrastructure, NaaS and services\n",
      "do not exist.\n",
      "Use case category Cat 1: describes a scenario related to core autonomous behaviour itself.\n",
      "Reference\n",
      "7.13.1 Use case requirements\n",
      "Critical requirements\n",
      "â AN-UC013-REQ-001: It is critical that autonomous networks (AN) enable plug and play of\n",
      "network functions in the underlay and subsequent seamless participation of such network functions\n",
      "in the AN functions.\n",
      "NOTE â Examples of AN functions are creation and hosting of controllers. Plug and play may be executed\n",
      "by manual or autonomous mechanisms.\n",
      "7.13.2 Use case specific figures\n",
      "None.\n",
      "7.14 âGenerative adversarial Sandboxâ: (or hybrid closed loops)\n",
      "Use case id FG-AN-usecase-014\n",
      "Use case name âGenerative adversarial Sandboxâ: (or hybrid closed loops)\n",
      "Base contribution FGAN-I-017\n",
      "Creation date 13/Apr/2021 \n",
      "\n",
      "Source:  train data.txt\n",
      "Text:  OR\n",
      "Top down bootstrapping of apps, services, NaaS, infrastructure, based on these\n",
      "new SRCs.\n",
      "Open issues E2E automation frameworks for composition of infrastructure, NaaS and services\n",
      "do not exist.\n",
      "Use case category Cat 1: describes a scenario related to core autonomous behaviour itself.\n",
      "Reference\n",
      "7.13.1 Use case requirements\n",
      "Critical requirements\n",
      "â AN-UC013-REQ-001: It is critical that autonomous networks (AN) enable plug and play of\n",
      "network functions in the underlay and subsequent seamless participation of such network functions\n",
      "in the AN functions.\n",
      "NOTE â Examples of AN functions are creation and hosting of controllers. Plug and play may be executed\n",
      "by manual or autonomous mechanisms.\n",
      "7.13.2 Use case specific figures\n",
      "None.\n",
      "7.14 âGenerative adversarial Sandboxâ: (or hybrid closed loops)\n",
      "Use case id FG-AN-usecase-014\n",
      "Use case name âGenerative adversarial Sandboxâ: (or hybrid closed loops)\n",
      "Base contribution FGAN-I-017\n",
      "Creation date 13/Apr/2021 \n",
      "\n",
      "Source:  train data.txt\n",
      "Text:  deployed in networks. In order to achieve mature solutions for autonomous\n",
      "network control, it will be essential for AN vendors to validate in advance that\n",
      "their products will operate successfully in the target customer networks, before\n",
      "they are actually deployed.\n",
      "A DT can be used to estimate accurately the resulting network performance of\n",
      "an experimentation approach and the effect after applying the actions produced\n",
      "by the AN, thus determining what network scenarios are well-supported by the\n",
      "product. After a comprehensive validation test, the vendor can apply the\n",
      "adaptations to the network.\n",
      "Error! Reference source not found. (2021-10) 51\n",
      "Following are examples of scenarios in this use case:\n",
      "Scenario-1: preparation of DT: import network configurations (may include\n",
      "closed loops?) into digital twin. This sets the stage for preparation of\n",
      "simulations in the DT.\n",
      "Scenario-2: trigger of DT for simulations: update of network configurations \n",
      "\n",
      "Source:  train data.txt\n",
      "Text:  deployed in networks. In order to achieve mature solutions for autonomous\n",
      "network control, it will be essential for AN vendors to validate in advance that\n",
      "their products will operate successfully in the target customer networks, before\n",
      "they are actually deployed.\n",
      "A DT can be used to estimate accurately the resulting network performance of\n",
      "an experimentation approach and the effect after applying the actions produced\n",
      "by the AN, thus determining what network scenarios are well-supported by the\n",
      "product. After a comprehensive validation test, the vendor can apply the\n",
      "adaptations to the network.\n",
      "Error! Reference source not found. (2021-10) 51\n",
      "Following are examples of scenarios in this use case:\n",
      "Scenario-1: preparation of DT: import network configurations (may include\n",
      "closed loops?) into digital twin. This sets the stage for preparation of\n",
      "simulations in the DT.\n",
      "Scenario-2: trigger of DT for simulations: update of network configurations \n",
      "\n"
     ]
    }
   ],
   "source": [
    "docs = vectordb.similarity_search(query)\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Retrieved documents: {len(docs)}\")\n",
    "for doc in docs:\n",
    "    doc_details = doc.to_json()['kwargs']\n",
    "    print(\"Source: \", doc_details['metadata']['source'])\n",
    "    print(\"Text: \", doc_details['page_content'], \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "\n",
    "We used Langchain, ChromaDB and Llama 2 as a LLM to build a Retrieval Augmented Generation solution. For testing, we were using the latest State of the Union address from Jan 2023.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References  \n",
    "\n",
    "[1] Murtuza Kazmi, Using LLaMA 2.0, FAISS and LangChain for Question-Answering on Your Own Data, https://medium.com/@murtuza753/using-llama-2-0-faiss-and-langchain-for-question-answering-on-your-own-data-682241488476  \n",
    "\n",
    "[2] Patrick Lewis, Ethan Perez, et. al., Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks, https://browse.arxiv.org/pdf/2005.11401.pdf \n",
    "\n",
    "[3] Minhajul Hoque, Retrieval Augmented Generation: Grounding AI Responses in Factual Data, https://medium.com/@minh.hoque/retrieval-augmented-generation-grounding-ai-responses-in-factual-data-b7855c059322  \n",
    "\n",
    "[4] Fangrui Liu\t, Discover the Performance Gain with Retrieval Augmented Generation, https://thenewstack.io/discover-the-performance-gain-with-retrieval-augmented-generation/\n",
    "\n",
    "[5] Andrew, How to use Retrieval-Augmented Generation (RAG) with Llama 2, https://agi-sphere.com/retrieval-augmented-generation-llama2/   \n",
    "\n",
    "[6] Yogendra Sisodia, Retrieval Augmented Generation Using Llama2 And Falcon, https://medium.com/@scholarly360/retrieval-augmented-generation-using-llama2-and-falcon-ed26c7b14670   \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration file created at: /home/.jupyter/jupyter_notebook_config.py\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Specify the configuration directory and file\n",
    "config_dir = \"/home/.jupyter/\"\n",
    "config_file = os.path.join(config_dir, \"jupyter_notebook_config.py\")\n",
    "\n",
    "# Ensure the directory exists (it should, but just in case)\n",
    "os.makedirs(config_dir, exist_ok=True)\n",
    "\n",
    "# Create or write to the config file\n",
    "config_content = \"\"\"\n",
    "c.NotebookApp.shutdown_no_activity_timeout = 10800  # Set to 2 hours (7200 seconds)\n",
    "\"\"\"\n",
    "\n",
    "with open(config_file, 'w') as f:\n",
    "    f.write(config_content)\n",
    "\n",
    "print(f\"Configuration file created at: {config_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: flask in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (3.0.3)\n",
      "Requirement already satisfied: Werkzeug>=3.0.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from flask) (3.0.4)\n",
      "Requirement already satisfied: Jinja2>=3.1.2 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from flask) (3.1.4)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from flask) (2.2.0)\n",
      "Requirement already satisfied: click>=8.1.3 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from flask) (8.1.7)\n",
      "Requirement already satisfied: blinker>=1.6.2 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from flask) (1.8.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from Jinja2>=3.1.2->flask) (2.1.5)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0mhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Requirement already satisfied: pyngrok in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (7.2.0)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /root/miniconda3/envs/py3.10/lib/python3.10/site-packages (from pyngrok) (6.0.2)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
      " * Running on all addresses (0.0.0.0)\n",
      " * Running on http://127.0.0.1:5000\n",
      " * Running on http://172.17.0.3:5000\n",
      "\u001b[33mPress CTRL+C to quit\u001b[0m\n",
      "172.17.0.3 - - [08/Oct/2024 15:30:37] \"\u001b[33mGET / HTTP/1.1\u001b[0m\" 404 -\n",
      "172.17.0.3 - - [08/Oct/2024 15:30:41] \"\u001b[31m\u001b[1mGET /query HTTP/1.1\u001b[0m\" 405 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "172.17.0.3 - - [08/Oct/2024 15:34:40] \"POST /query HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "172.17.0.3 - - [08/Oct/2024 15:41:00] \"POST /query HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "172.17.0.3 - - [08/Oct/2024 15:49:31] \"POST /query HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "172.17.0.3 - - [08/Oct/2024 15:52:27] \"\u001b[31m\u001b[1mPOST /query HTTP/1.1\u001b[0m\" 400 -\n",
      "172.17.0.3 - - [08/Oct/2024 15:54:34] \"POST /query HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "172.17.0.3 - - [08/Oct/2024 15:59:51] \"POST /query HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "172.17.0.3 - - [08/Oct/2024 16:22:48] \"POST /query HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "172.17.0.3 - - [08/Oct/2024 16:27:16] \"POST /query HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install flask\n",
    "!pip install pyngrok\n",
    "\n",
    "from pyngrok import ngrok\n",
    "from flask import Flask, request, jsonify\n",
    "\n",
    "# Set your ngrok authtoken\n",
    "# ngrok.set_auth_token(\"2n2mw3PwI2JeIWWbW6dOx1Bwl5L_4Co5Hnaejk29ke6jvS3b7\")\n",
    "\n",
    "# Create a Flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/query', methods=['POST'])\n",
    "def query_model():\n",
    "    data = request.get_json()\n",
    "    query = data.get('query', '')\n",
    "    \n",
    "    if not query:\n",
    "        return jsonify({\"error\": \"No query provided\"}), 400\n",
    "\n",
    "    try:\n",
    "        # Generate response from the model (replace qa.run with actual logic)\n",
    "        response = qa.run(query)\n",
    "        # Optionally, test another function or logic here\n",
    "        # test_rag(qa, query)\n",
    "    except Exception as e:\n",
    "        response = str(e)\n",
    "    \n",
    "    return jsonify({\"id\" : \"winest_chatbot\", \"response\": response})\n",
    "\n",
    "# Run the Flask app\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(host='0.0.0.0', port=5000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 5833676,
     "sourceId": 9570720,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 735,
     "modelInstanceId": 3093,
     "sourceId": 4298,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30559,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
